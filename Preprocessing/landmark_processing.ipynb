{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXBRI6hVIAlK",
        "outputId": "18666450-03c1-45d6-a4b1-dd8654c40a4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.1/36.1 MB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q mediapipe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tA_m4yLsLvIw"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import mediapipe as mp\n",
        "import os\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "from typing import Tuple, List, Optional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28IAekUuMQyJ",
        "outputId": "f5b07dad-5579-4961-8e14-71ad069454e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NhAHw45wIL0c"
      },
      "outputs": [],
      "source": [
        "class SignLanguageProcessor:\n",
        "    def __init__(self, num_frames: int = 113):\n",
        "        # Initialize Mediapipe Holistic\n",
        "        self.mp_holistic = mp.solutions.holistic\n",
        "        self.holistic = self.mp_holistic.Holistic(\n",
        "            static_image_mode=False,\n",
        "            min_detection_confidence=0.3,\n",
        "            min_tracking_confidence=0.3\n",
        "        )\n",
        "        self.num_frames = num_frames  # Number of frames to standardize to\n",
        "\n",
        "    def process_dataset(self,\n",
        "                       train_path: str,\n",
        "                       test_path: str,\n",
        "                       output_base_folder: str,\n",
        "                       augment_training: bool = True,\n",
        "                       num_augmentations: int = 4) -> None:\n",
        "        \"\"\"\n",
        "        Main pipeline to process both training and testing datasets\n",
        "        \"\"\"\n",
        "        # Create output directories\n",
        "        output_base = Path(output_base_folder)\n",
        "        train_output = output_base / 'train'\n",
        "        test_output = output_base / 'test'\n",
        "\n",
        "        # Process training data with augmentation\n",
        "        print(\"Processing training data...\")\n",
        "        self._process_dataset_split(\n",
        "            train_path,\n",
        "            train_output,\n",
        "            is_training=True,\n",
        "            augment=augment_training,\n",
        "            num_augmentations=num_augmentations\n",
        "        )\n",
        "\n",
        "        # Process test data without augmentation\n",
        "        print(\"\\nProcessing test data...\")\n",
        "        self._process_dataset_split(\n",
        "            test_path,\n",
        "            test_output,\n",
        "            is_training=False\n",
        "        )\n",
        "\n",
        "    def _process_dataset_split(self,\n",
        "                             input_path: str,\n",
        "                             output_path: Path,\n",
        "                             is_training: bool = False,\n",
        "                             augment: bool = False,\n",
        "                             num_augmentations: int = 4) -> None:\n",
        "        \"\"\"\n",
        "        Process a single dataset split (train or test)\n",
        "        \"\"\"\n",
        "        input_path = Path(input_path)\n",
        "        output_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Process each word folder\n",
        "        for word_folder in tqdm(list(input_path.iterdir()), desc=\"Processing words\"):\n",
        "            if not word_folder.is_dir():\n",
        "                continue\n",
        "\n",
        "            word = word_folder.name\n",
        "            word_output = output_path / word\n",
        "            word_output.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "            # Process each video in the word folder\n",
        "            for video_file in word_folder.glob('*.mp4'):\n",
        "                # Process original video\n",
        "                processed_frames = self._process_video(str(video_file))\n",
        "                if processed_frames is not None:\n",
        "                    keypoints = self._extract_keypoints_sequence(processed_frames)\n",
        "\n",
        "                    # Reshape keypoints to (113, 225) format\n",
        "                    reshaped_keypoints = self._reshape_keypoints(keypoints)\n",
        "\n",
        "                    # Save original keypoints\n",
        "                    output_file = word_output / f\"{video_file.stem}.npy\"\n",
        "                    np.save(output_file, reshaped_keypoints)\n",
        "\n",
        "                    # Generate augmentations for training data\n",
        "                    if is_training and augment:\n",
        "                        for i in range(num_augmentations):\n",
        "                            aug_keypoints = self._augment_landmarks(keypoints)\n",
        "                            aug_reshaped = self._reshape_keypoints(aug_keypoints)\n",
        "                            aug_output_file = word_output / f\"{video_file.stem}_aug_{i+1}.npy\"\n",
        "                            np.save(aug_output_file, aug_reshaped)\n",
        "\n",
        "    def _reshape_keypoints(self, keypoints: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Reshape keypoints from (num_frames, 75, 3) to (113, 225) format\n",
        "        75 landmarks = 33 pose + 21 left hand + 21 right hand\n",
        "        113 = number of selected frames\n",
        "        225 = 75 landmarks * 3 coordinates\n",
        "        \"\"\"\n",
        "        # First ensure we have exactly num_frames frames\n",
        "        if len(keypoints) != self.num_frames:\n",
        "            indices = np.linspace(0, len(keypoints)-1, self.num_frames, dtype=int)\n",
        "            keypoints = keypoints[indices]\n",
        "\n",
        "        # Reshape to 113 frames\n",
        "        keypoints_113 = np.zeros((113, 225))  # Initialize with zeros\n",
        "\n",
        "        # Fill the available frames (up to min(113, num_frames))\n",
        "        num_frames_to_use = min(113, self.num_frames)\n",
        "\n",
        "        # Reshape each frame's keypoints to a 1D array of 225 values\n",
        "        for i in range(num_frames_to_use):\n",
        "            flat_keypoints = keypoints[i].reshape(-1)  # Flatten 75x3 to 225\n",
        "            keypoints_113[i] = flat_keypoints\n",
        "\n",
        "        return keypoints_113\n",
        "\n",
        "    def _process_video(self, video_path: str) -> Optional[List[np.ndarray]]:\n",
        "        \"\"\"\n",
        "        Process a single video: trim and standardize frames\n",
        "        \"\"\"\n",
        "        try:\n",
        "            cap = cv2.VideoCapture(video_path)\n",
        "            frames = []\n",
        "            while cap.isOpened():\n",
        "                ret, frame = cap.read()\n",
        "                if not ret:\n",
        "                    break\n",
        "                frames.append(frame)\n",
        "            cap.release()\n",
        "\n",
        "            if not frames:\n",
        "                return None\n",
        "\n",
        "            # Calculate motion scores for trimming\n",
        "            motion_scores = []\n",
        "            for i in range(1, len(frames)):\n",
        "                diff = cv2.absdiff(frames[i-1], frames[i])\n",
        "                motion_scores.append(np.mean(diff))\n",
        "\n",
        "            # Trim based on motion\n",
        "            threshold = np.mean(motion_scores) * 0.3\n",
        "            start_idx = next((i for i, score in enumerate(motion_scores)\n",
        "                            if score > threshold), 0)\n",
        "            end_idx = len(frames) - next((i for i, score in enumerate(reversed(motion_scores))\n",
        "                                        if score > threshold), 0)\n",
        "\n",
        "            # Apply trimming\n",
        "            frames = frames[max(0, start_idx-5):min(len(frames), end_idx+5)]\n",
        "\n",
        "            # Standardize length\n",
        "            if frames:\n",
        "                indices = np.linspace(0, len(frames)-1, self.num_frames, dtype=int)\n",
        "                frames = [frames[i] for i in indices]\n",
        "                return frames\n",
        "\n",
        "            return None\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing video {video_path}: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def _extract_keypoints_sequence(self, frames: List[np.ndarray]) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Extract keypoints from a sequence of frames\n",
        "        \"\"\"\n",
        "        keypoints_sequence = []\n",
        "\n",
        "        for frame in frames:\n",
        "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            results = self.holistic.process(frame_rgb)\n",
        "\n",
        "            # Initialize keypoints arrays\n",
        "            pose_keypoints = np.zeros((33, 3))\n",
        "            left_hand_keypoints = np.zeros((21, 3))\n",
        "            right_hand_keypoints = np.zeros((21, 3))\n",
        "\n",
        "            # Extract pose landmarks if detected\n",
        "            if results.pose_landmarks:\n",
        "                pose_keypoints = np.array([[lm.x, lm.y, lm.z]\n",
        "                                         for lm in results.pose_landmarks.landmark])\n",
        "\n",
        "            # Extract hand landmarks if detected\n",
        "            if results.left_hand_landmarks:\n",
        "                left_hand_keypoints = np.array([[lm.x, lm.y, lm.z]\n",
        "                                              for lm in results.left_hand_landmarks.landmark])\n",
        "            if results.right_hand_landmarks:\n",
        "                right_hand_keypoints = np.array([[lm.x, lm.y, lm.z]\n",
        "                                               for lm in results.right_hand_landmarks.landmark])\n",
        "\n",
        "            # Combine all keypoints\n",
        "            frame_keypoints = np.concatenate([\n",
        "                pose_keypoints,\n",
        "                left_hand_keypoints,\n",
        "                right_hand_keypoints\n",
        "            ])\n",
        "            keypoints_sequence.append(frame_keypoints)\n",
        "\n",
        "        return np.array(keypoints_sequence)\n",
        "\n",
        "    def _augment_landmarks(self, landmarks: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Apply augmentation to landmarks sequence\n",
        "        \"\"\"\n",
        "        angle = random.uniform(-10, 10) if random.random() < 0.7 else 0\n",
        "        noise_std = 0.01\n",
        "\n",
        "        augmented_frames = []\n",
        "        for frame in landmarks:\n",
        "            augmented = frame.copy()\n",
        "\n",
        "            # Apply rotation\n",
        "            if angle != 0:\n",
        "                augmented[:, :2] = self._rotate_landmarks(augmented, angle_deg=angle)\n",
        "\n",
        "            # Apply noise\n",
        "            if random.random() < 0.5:\n",
        "                augmented = self._add_noise(augmented, noise_std=noise_std)\n",
        "\n",
        "            augmented_frames.append(augmented)\n",
        "\n",
        "        return np.array(augmented_frames)\n",
        "\n",
        "    def _rotate_landmarks(self, landmarks: np.ndarray, angle_deg: float) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Rotate landmarks in X-Y plane\n",
        "        \"\"\"\n",
        "        angle_rad = np.radians(angle_deg)\n",
        "        rotation_matrix = np.array([\n",
        "            [np.cos(angle_rad), -np.sin(angle_rad)],\n",
        "            [np.sin(angle_rad), np.cos(angle_rad)]\n",
        "        ])\n",
        "        return landmarks[:, :2] @ rotation_matrix.T\n",
        "\n",
        "    def _add_noise(self, landmarks: np.ndarray, noise_std: float) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Add Gaussian noise to landmarks\n",
        "        \"\"\"\n",
        "        noise = np.random.normal(0, noise_std, landmarks.shape)\n",
        "        return landmarks + noise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVeOk3O7L400",
        "outputId": "2a001729-5363-4fc3-a86f-6c34556fcccc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing training data...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing words: 100%|██████████| 30/30 [58:47<00:00, 117.59s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing test data...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing words: 100%|██████████| 30/30 [14:40<00:00, 29.34s/it]\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Define paths\n",
        "    TRAIN_PATH = \"/content/drive/MyDrive/Omdena Jakarta/datasets/original_dataset/train\"\n",
        "    TEST_PATH = \"/content/drive/MyDrive/Omdena Jakarta/datasets/original_dataset/test\"\n",
        "    OUTPUT_BASE_FOLDER = \"/content/drive/MyDrive/Omdena Jakarta/datasets/processed_dataset/sign_landmark_dataset\"\n",
        "\n",
        "    # Initialize and run processor\n",
        "    processor = SignLanguageProcessor()\n",
        "    processor.process_dataset(\n",
        "        train_path=TRAIN_PATH,\n",
        "        test_path=TEST_PATH,\n",
        "        output_base_folder=OUTPUT_BASE_FOLDER,\n",
        "        augment_training=True,\n",
        "        num_augmentations=4\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQRAhiKjUv5b"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}