{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "from tensorflow.keras.optimizers import AdamW"
      ],
      "metadata": {
        "id": "7YuHvRe5fVtZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Landmark Data Merging\n",
        "Previously, landmark data was stored as individual NumPy files (one per video). <br>\n",
        "This section merges all individual files into a single NumPy array (\"merged_data\") to significantly improve loading speed for multiple experiments. <br>\n",
        "This avoids repeated file I/O, resulting in faster iteration during development. <br>\n",
        "If you don't need to save the merged data, you can use the \"merged_data\" variable directly."
      ],
      "metadata": {
        "id": "PgbI1Odf49P1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_npy_files(parent_folder, output_folder, output_file_name):\n",
        "    \"\"\"Merges landmark data from individual .npy files into a single .npy file.\n",
        "    Args:\n",
        "        parent_folder: Path to the parent directory containing label subfolders.\n",
        "        output_folder: Path to the directory where the merged data will be saved.\n",
        "        output_file_name: Name of the output .npy file.\n",
        "    Note:\n",
        "        If you don't need to save the merged data to disk, you can modify the function\n",
        "        to directly return the 'merged_data' list instead of saving it.\n",
        "    \"\"\"\n",
        "    # Initialize a list to store the merged data. Each element is a dictionary\n",
        "    merged_data = []\n",
        "\n",
        "    # Iterate through each label subfolder in the parent folder.\n",
        "    for label in os.listdir(parent_folder):\n",
        "        label_folder = os.path.join(parent_folder, label)\n",
        "\n",
        "        # Check if the current item is a directory (a label folder).\n",
        "        if os.path.isdir(label_folder):\n",
        "            # Iterate through each file in the label folder.\n",
        "            for file in os.listdir(label_folder):\n",
        "                # Check if the file is a .npy file.\n",
        "                if file.endswith('.npy'):\n",
        "                    file_path = os.path.join(label_folder, file)\n",
        "                    # Load the .npy data file.\n",
        "                    data = np.load(file_path)\n",
        "                    # Append a dictionary to the merged_data list.  This dictionary contains:\n",
        "                    # - data: The loaded NumPy array.\n",
        "                    # - label: The label of the data (the name of the subfolder).\n",
        "                    # - file_name: The name of the original .npy file.\n",
        "                    merged_data.append({'data': data, 'label': label, 'file_name': file})\n",
        "\n",
        "    # Create the output directory if it doesn't exist.\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    # Save the merged data to a new .npy file.\n",
        "    np.save(os.path.join(output_folder, output_file_name), merged_data)"
      ],
      "metadata": {
        "id": "Wlk9E1FSdruc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parent_folder = '/content/drive/MyDrive/Omdena/sign_language_recognition/train'\n",
        "output_folder = '/content/drive/MyDrive/Omdena/sign_language_recognition'\n",
        "output_file_name = 'train.npy'\n",
        "merge_npy_files(parent_folder, output_folder, output_file_name)"
      ],
      "metadata": {
        "id": "eTdKVL8T5M0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parent_folder = '/content/drive/MyDrive/Omdena/sign_language_recognition/test'\n",
        "output_folder = '/content/drive/MyDrive/Omdena/sign_language_recognition'\n",
        "output_file_name = 'test.npy'\n",
        "merge_npy_files(parent_folder, output_folder, output_file_name)"
      ],
      "metadata": {
        "id": "Gt6nrsc75She"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation"
      ],
      "metadata": {
        "id": "FljaJVPwzNr0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loading"
      ],
      "metadata": {
        "id": "UZaAf7ZMf7Tx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-ui9rEgNe5m",
        "outputId": "08548e4f-ee71-40ee-83ce-93920c99b083"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = np.load('/content/drive/MyDrive/Omdena/sign_language_recognition/train.npy', allow_pickle=True)\n",
        "# Extract features (X) and labels (y)\n",
        "X_train = [item['data'] for item in train_data]\n",
        "y_train = [item['label'] for item in train_data]\n",
        "\n",
        "# # Convert to NumPy arrays\n",
        "X_train = np.array(X_train)\n",
        "y_train = np.array(y_train)\n",
        "\n",
        "test_data = np.load('/content/drive/MyDrive/Omdena/sign_language_recognition/test.npy', allow_pickle=True)\n",
        "# Extract features (X) and labels (y)\n",
        "X_test = [item['data'] for item in test_data]\n",
        "y_test = [item['label'] for item in test_data]\n",
        "\n",
        "# Convert to NumPy arrays\n",
        "X_test = np.array(X_test)\n",
        "y_test = np.array(y_test)"
      ],
      "metadata": {
        "id": "hCsfns5o0jiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoding\n",
        "Encode labels to integer for model input. A reversed dictionary is also created for decoding during deployment.\n"
      ],
      "metadata": {
        "id": "fBt1NFZdiRz9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Fit and transform the labels\n",
        "y_train = label_encoder.fit_transform(y_train)\n",
        "y_test = label_encoder.transform(y_test)\n",
        "\n",
        "label_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
        "print(\"Label Mapping:\", label_mapping)\n",
        "\n",
        "reverse_label_mapping = {int(v): str(k) for k, v in label_mapping.items()}\n",
        "print(\"Reverse Label Mapping\", reverse_label_mapping)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqRyfdv4NsKQ",
        "outputId": "13f52b1d-b18b-4a5d-b9fe-cc5d554e452f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label Mapping: {'adik': 0, 'anak': 1, 'besar': 2, 'buka': 3, 'buruk': 4, 'dengar': 5, 'gembira': 6, 'guru': 7, 'haus': 8, 'ibu': 9, 'jalan': 10, 'keluarga': 11, 'kertas': 12, 'kucing': 13, 'lapar': 14, 'lihat': 15, 'maaf': 16, 'main': 17, 'makan': 18, 'marah': 19, 'minum': 20, 'nama': 21, 'orang': 22, 'panggil': 23, 'rumah': 24, 'sedikit': 25, 'selamat': 26, 'senyum': 27, 'teman': 28, 'tidur': 29}\n",
            "Reverse Label Mapping {0: 'adik', 1: 'anak', 2: 'besar', 3: 'buka', 4: 'buruk', 5: 'dengar', 6: 'gembira', 7: 'guru', 8: 'haus', 9: 'ibu', 10: 'jalan', 11: 'keluarga', 12: 'kertas', 13: 'kucing', 14: 'lapar', 15: 'lihat', 16: 'maaf', 17: 'main', 18: 'makan', 19: 'marah', 20: 'minum', 21: 'nama', 22: 'orang', 23: 'panggil', 24: 'rumah', 25: 'sedikit', 26: 'selamat', 27: 'senyum', 28: 'teman', 29: 'tidur'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Label Smoothing\n",
        "\n",
        "### Label Smoothing\n",
        "\n",
        "Label smoothing is a regularization technique used to prevent overfitting, particularly in classification tasks. Instead of using \"hard\" targets (e.g., [0, 0, 1, 0]) it uses \"soft\" targets where a small probability mass is distributed across all classes.\n",
        "\n",
        "For example, with a smoothing factor of 0.1 and 4 classes, a true label of class 2 would be transformed from [0, 0, 1, 0] to [0.025, 0.025, 0.925, 0.025]."
      ],
      "metadata": {
        "id": "TefkJj8MiUYL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def smooth_labels(y_true, num_classes, smoothing=0.1):\n",
        "    \"\"\"Applies label smoothing to one-hot encoded labels.\n",
        "\n",
        "    Args:\n",
        "        y_true: True labels (integer, encoded labels).\n",
        "        num_classes: Total number of classes.\n",
        "        smoothing: Smoothing factor (0.0 for no smoothing, 1.0 for maximum smoothing).\n",
        "\n",
        "    Returns:\n",
        "        Smoothed labels (one-hot encoded).\n",
        "    \"\"\"\n",
        "    # Convert to one-hot\n",
        "    y_true_one_hot = tf.one_hot(y_true, depth=num_classes)\n",
        "    # Apply smoothing\n",
        "    smoothed_labels = y_true_one_hot * (1 - smoothing) + (smoothing / num_classes)\n",
        "    return smoothed_labels\n",
        "\n",
        "# Apply label smoothing to the training labels.\n",
        "num_classes = 30 # Number of classes in dataset\n",
        "label_smoothing = 0.1 # Smoothing parameter\n",
        "y_train_smoothed = smooth_labels(y_train, num_classes, smoothing=label_smoothing)\n",
        "\n",
        "# Convert y_test into one-hot format too, for consistency. We don't smooth the test labels.\n",
        "y_test = tf.one_hot(y_test, depth=num_classes)"
      ],
      "metadata": {
        "id": "hp1YoGWb2Bv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Angle Feature Engineering\n",
        "\n",
        "This section focuses on extracting relevant features from the landmark data in the form of angles between keypoints. These angles capture the relative positions and movements of body parts, particularly hands, wrists, and elbows. This information is crucial for the model to understand gestures and signs.\n",
        "\n",
        "For example, the angle between the wrist, elbow, and shoulder can indicate whether the hand is open or closed. Similarly, the angles between different hand keypoints can reveal finger movements and their relative positions.\n",
        "\n",
        "By including these angle features, the model learns to focus on the key aspects of hand and body posture that are essential for sign language recognition."
      ],
      "metadata": {
        "id": "vkJgz0QsiYjM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_angle(A, B, C):\n",
        "  \"\"\"Calculates the angle between three points in 3D space (in radians).\n",
        "\n",
        "  Args:\n",
        "      A: First point coordinates (3D numpy array).\n",
        "      B: Second point coordinates (3D numpy array).\n",
        "      C: Third point coordinates (3D numpy array).\n",
        "\n",
        "  Returns:\n",
        "      The angle between points A, B, and C (radians).\n",
        "  \"\"\"\n",
        "    BA = A - B\n",
        "    BC = C - B\n",
        "    # Compute dot product and magnitudes\n",
        "    dot_product = np.dot(BA, BC)\n",
        "    magnitude_BA = np.linalg.norm(BA)\n",
        "    magnitude_BC = np.linalg.norm(BC)\n",
        "\n",
        "    # Prevent division by zero\n",
        "    if magnitude_BA == 0 or magnitude_BC == 0:\n",
        "        return 0.0\n",
        "\n",
        "    # Calculate and clip cosine of the angle\n",
        "    cos_angle = dot_product / (magnitude_BA * magnitude_BC)\n",
        "    cos_angle = np.clip(cos_angle, -1.0, 1.0)\n",
        "\n",
        "    # Return the angle in radians\n",
        "    return np.arccos(cos_angle)"
      ],
      "metadata": {
        "id": "-MTGXkf609hS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_angles(X):\n",
        "  \"\"\"Extracts angles between keypoints for each video and frame.\n",
        "\n",
        "  Args:\n",
        "      X: Landmark data (numpy array, shape: (n_videos, n_frames, n_keypoints * 3)).\n",
        "\n",
        "  Returns:\n",
        "      Angles for each video and frame (numpy array, shape: (n_videos, n_frames, n_angles)).\n",
        "  \"\"\"\n",
        "\n",
        "    # Reshape the data into (n_videos, n_frames, n_keypoints, 3)\n",
        "    n_videos, n_frames, n_features = X.shape\n",
        "    n_keypoints = 75\n",
        "    X_reshaped = X.reshape(n_videos, n_frames, n_keypoints, 3)\n",
        "\n",
        "    # Define keypoints for angle calculation (indices start from 0)\n",
        "    pose_angle_indices = [\n",
        "        (12, 14, 16),\n",
        "        (14, 16, 18),\n",
        "        (18, 16, 22),\n",
        "        (14, 12, 24),\n",
        "\n",
        "        (11, 13, 15),\n",
        "        (13, 15, 17),\n",
        "        (17, 15, 21),\n",
        "        (13, 11, 23),\n",
        "    ]\n",
        "    # For both left and right hands\n",
        "    hand_angle_indices = [(4, 0, 8),\n",
        "                          (8, 0, 16),\n",
        "                          (0, 9, 12),\n",
        "                          (0, 17,20),\n",
        "                          ]\n",
        "\n",
        "    # Calculate angles for each video and frame\n",
        "    angles_list = []\n",
        "    for video in X_reshaped:\n",
        "        video_angles = []\n",
        "        for frame in video:\n",
        "            frame_angles = []\n",
        "            # Pose angles\n",
        "            for (i, j, k) in pose_angle_indices:\n",
        "                frame_angles.append(calculate_angle(frame[i], frame[j], frame[k]))\n",
        "            # Left hand angles\n",
        "            for (i, j, k) in hand_angle_indices:\n",
        "                frame_angles.append(calculate_angle(frame[33 + i], frame[33 + j], frame[33 + k]))\n",
        "            # Right hand angles\n",
        "            for (i, j, k) in hand_angle_indices:\n",
        "                frame_angles.append(calculate_angle(frame[54 + i], frame[54 + j], frame[54 + k]))\n",
        "            video_angles.append(frame_angles)\n",
        "        angles_list.append(video_angles)\n",
        "\n",
        "    # Convert angles list to a numpy array\n",
        "    angles_array = np.array(angles_list)  # Shape: (n_videos, n_frames, n_angles)\n",
        "    return angles_array"
      ],
      "metadata": {
        "id": "rJ2TO7za0_8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get joint angles\n",
        "X_train_angles = get_angles(X_train)\n",
        "X_test_angles = get_angles(X_test)"
      ],
      "metadata": {
        "id": "RQ190kdtk4Ez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Standardization\n",
        "\n",
        "### Standardization\n",
        "\n",
        "Standardization is applied to the landmark data to improve the performance of the transformer model. Transformers, like many neural networks, tend to perform better when input features have a mean of zero and a standard deviation of one. This process scales the data, preventing features with larger ranges from dominating the learning process.\n",
        "\n",
        "It's important to note that the angle features are added *after* the standardization of the original landmark data."
      ],
      "metadata": {
        "id": "fsuVjIr-if5q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create normalization layer. Standardization is performed per frame (axis=-1).\n",
        "normalization_layer = layers.Normalization(axis=-1)\n",
        "\n",
        "# Adapt the normalization layer to the training data to calculate mean and std\n",
        "normalization_layer.adapt(X_train)\n",
        "\n",
        "# Standardize the data\n",
        "X_train_standardized = normalization_layer(X_train).numpy()\n",
        "X_test_standardized = normalization_layer(X_test).numpy()\n",
        "\n",
        "# Add angle features to the standardized landmark data.\n",
        "X_train = np.concatenate([X_train_standardized, X_train_angles],axis=-1)\n",
        "X_test = np.concatenate([X_test_standardized, X_test_angles],axis=-1)\n",
        "\n",
        "# Convert to TensorFlow tensor\n",
        "X_train = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
        "X_test = tf.convert_to_tensor(X_test, dtype=tf.float32)\n",
        "y_train = tf.convert_to_tensor(y_train_smoothed, dtype=tf.float32)\n",
        "y_test = tf.convert_to_tensor(y_test, dtype=tf.float32)"
      ],
      "metadata": {
        "id": "qMr6Zc-F1AiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modeling\n",
        "\n",
        "This section defines and implements the transformer model used for sign language recognition. This specific architecture and its hyperparameters were selected based on previous experiments with various architectures and hyperparameter tuning."
      ],
      "metadata": {
        "id": "tAzicUYnzSsM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_transformer_model(input_shape, num_classes):\n",
        "    \"\"\"Creates a transformer model for sign language recognition.\n",
        "\n",
        "    Args:\n",
        "        input_shape: Shape of the input data (sequence length, feature dimension).\n",
        "        num_classes: Number of sign classes.\n",
        "\n",
        "    Returns:\n",
        "        A Keras Model instance.\n",
        "    \"\"\"\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "\n",
        "    # Add learnable positional information to the input sequence.\n",
        "    positional_encoding = layers.Embedding(input_dim=input_shape[0], output_dim=input_shape[1])(tf.range(input_shape[0]))\n",
        "    x = inputs + positional_encoding\n",
        "\n",
        "    # Transformer Encoder\n",
        "    for _ in range(4):  # 4 Transformer blocks\n",
        "        # Layer Normalization (applied before attention, based on prior experimentation)\n",
        "        x_norm = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "        # Multi-Head Attention (8 heads, increased key dimension to 128 from base 64)\n",
        "        attention_output = layers.MultiHeadAttention(num_heads=8, key_dim=128)(x_norm, x_norm)\n",
        "        x = x + attention_output\n",
        "        # Feed-Forward Network (added an extra dense layer with 512 units based on prior experimentation)\n",
        "        ff_output = layers.Dense(512, activation='relu')(x)\n",
        "        ff_output = layers.Dense(241, activation='relu')(x)\n",
        "        x = layers.LayerNormalization(epsilon=1e-6)(x + ff_output)\n",
        "\n",
        "    # Global Average Pooling\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "\n",
        "    # Output Layer\n",
        "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    return models.Model(inputs, outputs)"
      ],
      "metadata": {
        "id": "PwJ0qlrO1EfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Configuration\n",
        "\n",
        "This section details the configuration used for training the transformer model. The hyperparameters, optimizer, and callbacks were chosen based on previous experiments and exploration.\n",
        "\n",
        "- **Optimizer**: AdamW optimizer is used with a learning rate of 0.001 and a weight decay of 1e-4. Weight decay helps prevent overfitting by penalizing large weights during training.\n",
        "\n",
        "**Learning Rate Scheduler (ReduceLROnPlateau):**\n",
        "\n",
        "This callback monitors the validation loss and reduces the learning rate by a factor of 0.5 if the validation loss remains stagnant for 3 epochs (patience).\n",
        "\n",
        "**Early Stopping:**\n",
        "\n",
        "Early stopping terminates training if the validation loss does not improve for 5 epochs (patience). This prevents overfitting by stopping training when the model starts to memorize the training data instead of generalizing to unseen data."
      ],
      "metadata": {
        "id": "rlmYWkC_IX8X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = (113, 241)  # (n_frames, n_keypoints * n_coordinates + n_angles)\n",
        "num_classes = 30\n",
        "\n",
        "transformer_model = create_transformer_model(input_shape, num_classes)\n",
        "\n",
        "# Compile the model\n",
        "transformer_model.compile(\n",
        "    optimizer=AdamW(learning_rate=0.001, weight_decay=1e-4),\n",
        "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "# Add the learning rate scheduler callback\n",
        "lr_callback = tf.keras.callbacks.ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.5,\n",
        "    patience=3,\n",
        "    min_lr=1e-6\n",
        ")\n",
        "# Add early stopping callback\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=5,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "# Combine Callbacks\n",
        "callbacks = [lr_callback, early_stopping]\n",
        "\n",
        "model = transformer_model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_test, y_test),\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    callbacks=[callbacks]\n",
        ")"
      ],
      "metadata": {
        "id": "w8VNlwu1wR8M",
        "outputId": "3a7366f2-1082-40d5-a41b-5b50d5cf0402",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 480ms/step - accuracy: 0.2674 - loss: 3.0585 - val_accuracy: 0.4667 - val_loss: 1.6266 - learning_rate: 0.0010\n",
            "Epoch 2/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step - accuracy: 0.6727 - loss: 1.4705 - val_accuracy: 0.5833 - val_loss: 1.3509 - learning_rate: 0.0010\n",
            "Epoch 3/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step - accuracy: 0.8427 - loss: 1.1524 - val_accuracy: 0.6833 - val_loss: 1.1439 - learning_rate: 0.0010\n",
            "Epoch 4/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 53ms/step - accuracy: 0.9605 - loss: 0.8573 - val_accuracy: 0.7167 - val_loss: 1.0176 - learning_rate: 0.0010\n",
            "Epoch 5/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 53ms/step - accuracy: 0.9856 - loss: 0.7640 - val_accuracy: 0.6667 - val_loss: 1.2299 - learning_rate: 0.0010\n",
            "Epoch 6/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step - accuracy: 0.9431 - loss: 0.8771 - val_accuracy: 0.7333 - val_loss: 0.9796 - learning_rate: 0.0010\n",
            "Epoch 7/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 52ms/step - accuracy: 0.9906 - loss: 0.7465 - val_accuracy: 0.7833 - val_loss: 0.9593 - learning_rate: 0.0010\n",
            "Epoch 8/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 50ms/step - accuracy: 0.9858 - loss: 0.7280 - val_accuracy: 0.7167 - val_loss: 1.2052 - learning_rate: 0.0010\n",
            "Epoch 9/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 51ms/step - accuracy: 0.9142 - loss: 0.9790 - val_accuracy: 0.5500 - val_loss: 1.4969 - learning_rate: 0.0010\n",
            "Epoch 10/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 52ms/step - accuracy: 0.8706 - loss: 1.1071 - val_accuracy: 0.5333 - val_loss: 1.4291 - learning_rate: 0.0010\n",
            "Epoch 11/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 53ms/step - accuracy: 0.9581 - loss: 0.8671 - val_accuracy: 0.7500 - val_loss: 0.9720 - learning_rate: 5.0000e-04\n",
            "Epoch 12/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 50ms/step - accuracy: 0.9973 - loss: 0.7066 - val_accuracy: 0.8333 - val_loss: 0.8660 - learning_rate: 5.0000e-04\n",
            "Epoch 13/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step - accuracy: 1.0000 - loss: 0.6742 - val_accuracy: 0.8167 - val_loss: 0.8797 - learning_rate: 5.0000e-04\n",
            "Epoch 14/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 49ms/step - accuracy: 1.0000 - loss: 0.6638 - val_accuracy: 0.8167 - val_loss: 0.8892 - learning_rate: 5.0000e-04\n",
            "Epoch 15/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step - accuracy: 1.0000 - loss: 0.6597 - val_accuracy: 0.8167 - val_loss: 0.8928 - learning_rate: 5.0000e-04\n",
            "Epoch 16/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 51ms/step - accuracy: 1.0000 - loss: 0.6572 - val_accuracy: 0.8333 - val_loss: 0.8998 - learning_rate: 2.5000e-04\n",
            "Epoch 17/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 51ms/step - accuracy: 1.0000 - loss: 0.6561 - val_accuracy: 0.8167 - val_loss: 0.9008 - learning_rate: 2.5000e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the epoch with the best performance\n",
        "best_epoch = early_stopping.stopped_epoch - early_stopping.patience + 1\n",
        "\n",
        "# Print the metrics of the best epoch\n",
        "best_val_loss = model.history['val_loss'][best_epoch - 1]\n",
        "best_val_accuracy = model.history['val_accuracy'][best_epoch - 1]  # Change to the metric you're tracking\n",
        "\n",
        "print(f\"Best Epoch: {best_epoch}\")\n",
        "print(f\"Validation Loss: {best_val_loss}\")\n",
        "print(f\"Validation Accuracy: {best_val_accuracy}\")"
      ],
      "metadata": {
        "id": "jw7Ob5AYxFz-",
        "outputId": "dbd2df9b-fcaa-4e0f-d378-930e6705a875",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Epoch: 12\n",
            "Validation Loss: 0.8660455346107483\n",
            "Validation Accuracy: 0.8333333134651184\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deployment Preparation"
      ],
      "metadata": {
        "id": "kcmwtLhSuuVV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exporting the model"
      ],
      "metadata": {
        "id": "YueTp0RPiolK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model as TensorFlow SavedModel\n",
        "save_path = '/content/drive/MyDrive/Omdena/sign_language_recognition/transformer_model_b.keras'\n",
        "transformer_model.save(save_path)"
      ],
      "metadata": {
        "id": "cw23J5evud81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sanity check, whether it truly saved the best weight\n",
        "loaded_model = tf.keras.models.load_model(save_path)\n",
        "\n",
        "# Evaluate on validation data\n",
        "val_loss, val_accuracy = loaded_model.evaluate(X_test, y_test)\n",
        "\n",
        "print(f\"Validation Loss: {val_loss}\")\n",
        "print(f\"Validation Accuracy: {val_accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mE2WtB69Wvt-",
        "outputId": "4f6ea67b-145e-425b-aec3-dd44a46df241"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2s/step - accuracy: 0.8472 - loss: 0.8356\n",
            "Validation Loss: 0.8660455346107483\n",
            "Validation Accuracy: 0.8333333134651184\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exporting Training Statistics for Standardization"
      ],
      "metadata": {
        "id": "ACzX_cykiuH8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean = normalization_layer.mean.numpy()\n",
        "variance = normalization_layer.variance.numpy()\n",
        "\n",
        "# Save mean and variance to a dictionary\n",
        "normalization_stats = {\n",
        "    \"mean\": mean.tolist(),\n",
        "    \"variance\": variance.tolist()\n",
        "}\n",
        "\n",
        "# Specify the Google Drive path\n",
        "save_path = \"/content/drive/MyDrive/Omdena/sign_language_recognition/normalization_stats.json\"\n",
        "\n",
        "# Save the normalization_stats to Google Drive\n",
        "with open(save_path, \"w\") as json_file:\n",
        "    json.dump(normalization_stats, json_file)"
      ],
      "metadata": {
        "id": "FnXLCGBWZ1XZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Example inference\n",
        "# load_path = '/content/drive/MyDrive/Omdena/sign_language_recognition/normalization_stats.json'\n",
        "\n",
        "# # Load mean and variance from JSON\n",
        "# with open(load_path, \"r\") as f:\n",
        "#     normalization_stats = json.load(f)\n",
        "\n",
        "# mean = tf.convert_to_tensor(normalization_stats[\"mean\"])\n",
        "# variance = tf.convert_to_tensor(normalization_stats[\"variance\"])\n",
        "\n",
        "# # Create normalization function\n",
        "# def normalize_data(data, mean, variance):\n",
        "#     return (data - mean) / tf.sqrt(variance)"
      ],
      "metadata": {
        "id": "ykHNoeU2aRDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exporting Decoder"
      ],
      "metadata": {
        "id": "qBLZFKR3izLs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure all keys/values are standard Python types\n",
        "reverse_label_mapping = {int(v): str(k) for k, v in label_mapping.items()}\n",
        "\n",
        "# Specify the Google Drive path\n",
        "save_path = \"/content/drive/MyDrive/Omdena/sign_language_recognition/reverse_label_mapping.json\"\n",
        "\n",
        "# Save the decoder to Google Drive\n",
        "with open(save_path, \"w\") as json_file:\n",
        "    json.dump(reverse_label_mapping, json_file)"
      ],
      "metadata": {
        "id": "gkHx2kJ9ZyBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Example inference\n",
        "# load_path = \"/content/drive/MyDrive/Omdena/sign_language_recognition/reverse_label_mapping.json\"\n",
        "\n",
        "# with open(load_path, \"r\") as json_file:\n",
        "#     reverse_label_mapping = json.load(json_file)\n",
        "\n",
        "\n",
        "# predicted_index = 2  # This is the model's output (e.g., from `argmax`)\n",
        "# predicted_label = reverse_label_mapping[str(predicted_index)]  # Convert to string key for JSON compatibility\n",
        "\n",
        "# print(f\"Predicted Label: {predicted_label}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpriU5yYaUbE",
        "outputId": "84c5b839-04cf-485b-f2ad-55fb804c9015"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Label: besar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q dagshub\n",
        "\n",
        "from dagshub.notebook import save_notebook\n",
        "\n",
        "save_notebook(repo=\"Omdena/JakartaIndonesia_SignLanguageTranslation\", path=\"modeling\", branch=\"kenji_modeling\", commit_message=\"Add Final Landmark Transformer Model notebook\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJpKNZaLJsbs",
        "outputId": "baaa3ee7-372c-42e1-8ffa-ef1ff16ab4f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m255.6/255.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.2/203.2 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.2/83.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.0/74.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    }
  ]
}